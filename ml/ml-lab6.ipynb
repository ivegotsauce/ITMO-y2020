{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    cleaned_text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/SMS.tsv', sep='\\t')\n",
    "y = pd.get_dummies(df['class']).ham\n",
    "X = df.text.map(preprocess_text)\n",
    "nltk.download('stopwords')\n",
    "tfid = TfidfVectorizer(max_features=2000, stop_words=stopwords.words('english'))\n",
    "X = tfid.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = RandomForestClassifier().fit(X, y).feature_importances_\n",
    "features = tfid.get_feature_names_out()\n",
    "features_emb = sorted(zip(features, importances), key=lambda x: x[1], reverse=True)\n",
    "features_emb = [feature for feature, importance in features_emb if importance > np.mean(importances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305 2000\n"
     ]
    }
   ],
   "source": [
    "print(len(features_emb), len(features))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = X_train.toarray()\n",
    "X_ts = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(m, col):\n",
    "    matrix = []\n",
    "    for vec in m:\n",
    "        new_vec = []\n",
    "        for j in col:\n",
    "            new_vec.append(vec[j])\n",
    "        matrix.append(new_vec)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0\n",
    "best_features = []\n",
    "while len(best_features) < 30:\n",
    "    best_columns = best_features.copy()\n",
    "    for k, feature in enumerate(features):\n",
    "        print(k, best_features, len(best_features), best_accuracy)\n",
    "        if not k in best_features:\n",
    "            columns = best_columns.copy()\n",
    "            columns.append(k)\n",
    "            matrix = get_matrix(X_tr, columns)\n",
    "            model = GaussianNB()\n",
    "            model.fit(matrix, y_train)\n",
    "            matrix = get_matrix(X_ts, columns)\n",
    "            y_pred = model.predict(matrix)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            if acc > best_accuracy:\n",
    "                best_accuracy = acc\n",
    "                best_features = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_wr = [features[k] for k in best_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = []\n",
    "features_fl = []\n",
    "matrix = abs(np.corrcoef(X.toarray(), rowvar=False))\n",
    "for p in np.argwhere(matrix > matrix.flatten().mean()):\n",
    "    p =  tuple(p)\n",
    "    i, j = p\n",
    "    if i == j:\n",
    "        continue\n",
    "    if not i in features_fl and not i in filtered:\n",
    "        features_fl.append(i)\n",
    "        filtered.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_fm = [features[i] for i in range(len(features)) if not i in filtered]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['call', 'txt', 'free', 'mobile', 'claim', 'prize', 'text', 'win', 'stop', 'service', 'reply', 'urgent', 'nokia', 'customer', 'new', 'tone', 'contact', 'per', 'ringtone', 'guaranteed', 'cash', 'chat', 'awarded', 'pmin', 'apply', 'box', 'ppm', 'pobox', 'landline', 'draw']\n",
      "['free', 'reply', 'urgent', 'receive', 'mobile', 'im', 'txt', 'claim', 'experience', 'box', 'service', 'sat', 'hey', 'ill', 'login', 'also', 'apply', 'going', 'cancel', 'code', 'brought', 'bugis', 'age', 'attempt', 'auction', 'may', 'daily', 'put', 'lucky', 'sir']\n",
      "['aiyah', 'appreciate', 'argue', 'argument', 'arms', 'arrested', 'asap', 'asks', 'asleep', 'assume', 'ate', 'attend', 'bag', 'bahamas', 'bak', 'balance', 'barely', 'basic', 'basically', 'bath', 'bathe', 'bathing', 'battery', 'bay', 'bcums', 'bday', 'becoz', 'bed', 'bedroom', 'beer']\n"
     ]
    }
   ],
   "source": [
    "print(features_emb[:30])\n",
    "print(features_wr)\n",
    "print(features_fm[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ac' 'account' 'action' 'admirer' 'age' 'alert' 'ampm' 'announcement'\n",
      " 'anytime' 'apply' 'arcade' 'arrive' 'attempt' 'auction' 'await'\n",
      " 'awaiting' 'award' 'awarded' 'bid' 'bluetooth' 'bonus' 'box'\n",
      " 'btnationalrate' 'bx' 'call' 'caller' 'camcorder' 'camera' 'cash'\n",
      " 'cashbalance']\n",
      "['ac' 'access' 'admirer' 'age' 'apply' 'attempt' 'award' 'awarded' 'bid'\n",
      " 'box' 'call' 'camera' 'cant' 'cash' 'charity' 'chat' 'choose' 'claim'\n",
      " 'club' 'code' 'collect' 'collection' 'come' 'congrats' 'contact'\n",
      " 'content' 'cost' 'credits' 'currently' 'customer']\n",
      "['account' 'admirer' 'ae' 'age' 'ampm' 'announcement' 'ansr' 'apply'\n",
      " 'attempt' 'auction' 'await' 'awaiting' 'award' 'awarded' 'bluetooth'\n",
      " 'bonus' 'box' 'btnationalrate' 'bx' 'call' 'caller' 'camcorder' 'camera'\n",
      " 'cash' 'cashbalance' 'cashin' 'cc' 'cd' 'cds' 'chance']\n"
     ]
    }
   ],
   "source": [
    "model = SelectKBest(chi2, k=300)\n",
    "model.fit(X, y)\n",
    "print(features[model.get_support()][:30])\n",
    "X_new_chi2 = model.transform(X)\n",
    "model = SelectFromModel(LogisticRegression(penalty=\"l1\", dual=False, solver='liblinear').fit(X, y), prefit=True)\n",
    "print(features[model.get_support()][:30])\n",
    "X_new_reg = model.transform(X)\n",
    "model = SelectKBest(f_classif, k=300)\n",
    "model.fit(X, y)\n",
    "print(features[model.get_support()][:30])\n",
    "X_new_f = model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = pd.DataFrame(X.toarray(), columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_chi2_train, X_chi2_test, y_train, y_test = train_test_split(X_new_chi2, y, test_size=0.2, random_state=42)\n",
    "X_reg_train, X_reg_test, y_train, y_test = train_test_split(X_new_reg, y, test_size=0.2, random_state=42)\n",
    "X_f_train, X_f_test, y_train, y_test = train_test_split(X_new_f, y, test_size=0.2, random_state=42)\n",
    "X_emb_train, X_emb_test, y_train, y_test = train_test_split(X_[features_emb], y, test_size=0.2, random_state=42)\n",
    "X_fm_train, X_fm_test, y_train, y_test = train_test_split(X_[features_fm], y, test_size=0.2, random_state=42)\n",
    "X_wr_train, X_wr_test, y_train, y_test = train_test_split(X_[features_wr], y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "default: 0.8663677130044843\n",
      "chi2: 0.95695067264574\n",
      "l1-based selection: 0.9533632286995516\n",
      "f-test: 0.957847533632287\n",
      "random forest: 0.9560538116591928\n",
      "correlation: 0.9632286995515695\n",
      "Bayes wrapper: 0.9103139013452914\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression')\n",
    "print(f'default: {accuracy_score(y_test, LogisticRegression().fit(X_train, y_train).predict(X_test))}')\n",
    "print(f'chi2: {accuracy_score(y_test, LogisticRegression().fit(X_chi2_train, y_train).predict(X_chi2_test))}')\n",
    "print(f'l1-based selection: {accuracy_score(y_test, LogisticRegression().fit(X_reg_train, y_train).predict(X_reg_test))}')\n",
    "print(f'f-test: {accuracy_score(y_test, LogisticRegression().fit(X_f_train, y_train).predict(X_f_test))}')\n",
    "print(f'random forest: {accuracy_score(y_test, LogisticRegression().fit(X_emb_train, y_train).predict(X_emb_test))}')\n",
    "print(f'correlation: {accuracy_score(y_test, LogisticRegression().fit(X_fm_train, y_train).predict(X_fm_test))}')\n",
    "print(f'Bayes wrapper: {accuracy_score(y_test, LogisticRegression().fit(X_wr_train, y_train).predict(X_wr_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "default: 0.862780269058296\n",
      "chi2: 0.9381165919282511\n",
      "l1-based selection: 0.9542600896860987\n",
      "f-test: 0.9381165919282511\n",
      "random forest: 0.9345291479820628\n",
      "correlation: 0.9246636771300448\n",
      "Bayes wrapper: 0.947085201793722\n"
     ]
    }
   ],
   "source": [
    "print('KNN')\n",
    "print(f'default: {accuracy_score(y_test, KNeighborsClassifier().fit(X_train, y_train).predict(X_test))}')\n",
    "print(f'chi2: {accuracy_score(y_test, KNeighborsClassifier().fit(X_chi2_train, y_train).predict(X_chi2_test))}')\n",
    "print(f'l1-based selection: {accuracy_score(y_test, KNeighborsClassifier().fit(X_reg_train, y_train).predict(X_reg_test))}')\n",
    "print(f'f-test: {accuracy_score(y_test, KNeighborsClassifier().fit(X_f_train, y_train).predict(X_f_test))}')\n",
    "print(f'random forest: {accuracy_score(y_test, KNeighborsClassifier().fit(X_emb_train, y_train).predict(X_emb_test))}')\n",
    "print(f'correlation: {accuracy_score(y_test, KNeighborsClassifier().fit(X_fm_train, y_train).predict(X_fm_test))}')\n",
    "print(f'Bayes wrapper: {accuracy_score(y_test, KNeighborsClassifier().fit(X_wr_train, y_train).predict(X_wr_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree\n",
      "default: 0.7838565022421524\n",
      "chi2: 0.9623318385650225\n",
      "l1-based selection: 0.967713004484305\n",
      "f-test: 0.9659192825112107\n",
      "random forest: 0.957847533632287\n",
      "correlation: 0.9623318385650225\n",
      "Bayes wrapper: 0.9426008968609866\n"
     ]
    }
   ],
   "source": [
    "print('Decision tree')\n",
    "print(f'default: {accuracy_score(y_test, DecisionTreeClassifier().fit(X_train, y_train).predict(X_test))}')\n",
    "print(f'chi2: {accuracy_score(y_test, DecisionTreeClassifier().fit(X_chi2_train, y_train).predict(X_chi2_test))}')\n",
    "print(f'l1-based selection: {accuracy_score(y_test, DecisionTreeClassifier().fit(X_reg_train, y_train).predict(X_reg_test))}')\n",
    "print(f'f-test: {accuracy_score(y_test, DecisionTreeClassifier().fit(X_f_train, y_train).predict(X_f_test))}')\n",
    "print(f'random forest: {accuracy_score(y_test, DecisionTreeClassifier().fit(X_emb_train, y_train).predict(X_emb_test))}')\n",
    "print(f'correlation: {accuracy_score(y_test, DecisionTreeClassifier().fit(X_fm_train, y_train).predict(X_fm_test))}')\n",
    "print(f'Bayes wrapper: {accuracy_score(y_test, DecisionTreeClassifier().fit(X_wr_train, y_train).predict(X_wr_test))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
